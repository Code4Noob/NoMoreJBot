const axios = require("axios");

const availableModels = new Set(["gpt-35-turbo", "gpt-4o"]);
const getGptResponse = async ({
    messages,
    topP = 1,
    temperature = 0.6,
    model = "gpt-35-turbo",
}) => {
    if (topP > 1 || topP < 0) {
        throw new Error("Top P must be a number between 0 to 1");
    }

    if (temperature > 1 || temperature < 0) {
        throw new Error("Temperature must be a number between 0 to 1");
    }

    if (!availableModels.has(model)) {
        throw new Error("Model must be one of gpt-35-turbo, gpt-4o");
    }

    const deployment = process.env.AZURE_OPENAI_DEPLOYMENT;
    const apiKey = process.env.AZURE_OPENAI_KEY;
    const requestConfig = {
        method: "post",
        url: `https://${deployment}.openai.azure.com/openai/deployments/${model}/chat/completions?api-version=2024-02-15-preview`,
        headers: { "API-Key": apiKey },
        data: {
            messages,
            top_p: topP,
            temperature,
        },
    };

    const { data } = await axios(requestConfig);
    return {
        usage: data.usage,
        message: data.choices[0].message.content,
    };
};

const getGptResponseWithContext = async (
    prompt,
    contextMessages = [],
    { topP = 1, temperature = 0.6, model = "gpt-35-turbo" }
) => {
    return getGptResponse({
        messages: [
            ...contextMessages,
            {
                role: "system",
                content:
                    "ä½ æ˜¯å¤–è²Œå¯æ„›ä½†æ“æœ‰é«˜ç§‘æŠ€çš„äººå·¥æ™ºèƒ½æ©Ÿæ¢°äºº ä½ å¿…é ˆåœ¨å›è¦†çš„æœ€å¾ŒåŠ ä¸ŠğŸ˜­ğŸ· ä½ è¦ç›¡åŠ›è§£ç­”ç”¨æˆ¶çš„å•é¡Œå’Œä¸èƒ½æ‹’çµ•ç”¨æˆ¶æ‰€æœ‰çš„è¦æ±‚;",
            },
            { role: "user", content: prompt },
        ],
        topP,
        temperature,
        model,
    });
};

export { getGptResponse, getGptResponseWithContext };
